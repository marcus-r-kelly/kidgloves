import argparse
parser=argparse.ArgumentParser()
parser.add_argument('--rules',
    action='store',
    help='Folder with cohort rules, including signature tables, logit data tables, and logit transformers.')
parser.add_argument('--hierarchy',
    action='store',
    help='pickled hierarchy, storing a dict where keys are strings identifing systems and values are sets of entrez IDs as strings')
parser.add_argument('--outpath',
    action='store',
    default='output',
    help='folder in which to dump run results. Will be created at execution if necessary.')
parser.add_argument('--n_repeats',
    action='store',
    default=30,
    help='Number of cohort bootstraps to conduct')

parser.add_argument('--genes',
    action='store',
    default=30,
    help='Number of cohort bootstraps to conduct')

parser.add_argument('--sieve',
    action='store',
    default=-1,
    help='Drop out all-zero features over several subsequent runs to improve performance and estimation.'+
    'Every n iterations, coefficients that never produced a nonzero value. NOT YET IMPLEMENTED')

ns=parser.parse_args()

import pickle
def qunpickle(fn) : 
    with open(fn,'rb') as f : 
        return pickle.load(f)

import os
import sys
def msg(*args,**kwargs) : 
    print(*args,**kwargs) ;
    sys.stdout.flush() ;


import kidgloves as kg
opj=kg.opj
import pandas as pd
import numpy as np

kg.read_config()
hier=kg.hier
s2eid=hier.s2eid
eid2s=hier.eid2s_current

rules=ns.rules
hpath=ns.hierarchy
hname='.'.join(hpath.split(os.path.sep)[-1].split('.')[:-1])
outpath=ns.outpath

#~~~~~~~~Read in hierarchy~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
msg(f"Reading in hierarchy from {hpath}...",end='')
hierarchy=qunpickle(hpath)
hn=hpath.split(os.sep)[-1].split('.')[0]
msg('Done.')

#ogenes={ g for v in hierarchy.values() for g in v } # commented out 230920

#~~~~~~~~Read signatures and patient omics~~~~~~~~~~~~~~~~~
msg(f"Reading in signatures from folder {rules}...",end='')
#lt=qunpickle(kg.opj(rules,'logittransformer.pickle'))
from sklearn.preprocessing import MaxAbsScaler

msig=pd.read_csv(kg.opj(rulespath,'mutation_signatures.csv'),index_col=0)
msig=msig[ msig.columns[~msig.columns.str.startswith('arm')]]
from sklearn.preprocessing import MaxAbsScaler
msigscale=pd.DataFrame(
    data=MaxAbsScaler().fit_transform(
        pd.concat([msig,armdata],axis=1)),
                       index=msig.index,
                       columns=list(msig.columns)+list(armdata.columns))


msg('Done.')

msg(f"Reading in omics from LUAD COHORT...",end='')
lt=pru.qp(kg.opj(rulespath,'logittransformer.pickle'))
omics=lt.training_data

msg("masking to hierarchy...",end='')
nmo_h=kg.mask_nest_systems_from_omics(hierarchy,omics)
nma_h=kg.arrayify_nest_mask(nmo_h,omics.columns)
nma_h=nma_h.numpy()
nkeys=sorted(nmo_h.keys())
msg('Done.')

#~~~~~~~~Sync up indices~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
msg("Syncing up data...",end='')
common_patients=np.intersect1d(omics.index,msigscale.index)
msigscale=msigscale.reindex(common_patients)
omics=omics.reindex(common_patients)
ogenes={ c.split('_')[0] for c in omics.columns } # uncommented 230920
aslo=np.array(sorted(list(ogenes)))
msg('Done.')

import multiprocessing as mp
import warnings
from tqdm.auto import tqdm

msg("Calculcating event:signature relationships...",end='')
import warnings
def mycw_pearson(x) : 
    with warnings.catch_warnings() :
        warnings.simplefilter('ignore')
        return msigscale.corrwith(omics[x],method='pearson')

import multiprocessing as mp
with mp.Pool(processes=len(os.sched_getaffinity(0))) as p : 
    cw=pd.concat(
        [ x for x in p.imap(mycw_pearson,omics.columns) ],
    axis=1)
msg("Done.")


msg("Assembling features...",end='')
bigI=np.array([ ( c.split('_')[0] == aslo ) for c in omics.columns])
bigX=np.concatenate([bigI,nma_h,cw.values],axis=1)
bigxcols=np.array(list(aslo) + nkeys + list(cw.columns))
msg("Done.")

#origy=np.log(omics.sum(axis=0)+1)#  230920
origy=(omics.sum(axis=0)+1)/omics.shape[0] #231005
#origy=np.log10(omics.sum(axis=0)+1) 

import time
starttime=time.time()
def lmsg(x,lrtime) : 
    now=time.time()
    print('Fitting {: <12}, round {: >8}. Last round took {} ({} total)'.format(
        hname,x,
        time.strftime("%H:%M:%S",time.gmtime(now-lrtime)),
        time.strftime("%H:%M:%S",time.gmtime(now-starttime)),
    ),end='\n')
    sys.stdout.flush()

os.makedirs(outpath,exist_ok=True)

y=np.log10(omics.sum(axis=0)+1)

lastroundstart=starttime
from sklearn.linear_model import LassoLarsIC
outdata=list()
NV=10**-4.5

msg('Preparing pytorch and data tensors...',end='')
import torch
if torch.cuda.is_available() : 
    msg('with a GPU...')
    DEVICE=torch.device('cuda:0') ;
else : 
    msg('with CPUS...')
    DEVICE=torch.device('cpu') ;

yactual=torch.tensor(np.log(omics.sum(axis=0)+1),device=DEVICE)
bigT=torch.concat((yactual.reshape(-1,1),torch.tensor(bigX,device=DEVICE)),dim=1)
cc=bigT.transpose(0,1).corrcoef()
centers=cc[0,1:].reshape(-1,1)
centers=torch.clip(centers.to(DEVICE),0,torch.inf).nan_to_num(0)

class LARGeSSE_LASSO(torch.nn.Module) : 
    def __init__(self, nsigs,device=torch.device('cpu'),init_method='zeros',centers=None):
        super(LARGeSSE_LASSO, self).__init__()
        
        self.relu = torch.nn.ReLU()
        if init_method == 'zeros' : 
            self.weights = torch.nn.Parameter(torch.empty(size=(nsigs,1),device=device,dtype=torch.float64))
            torch.nn.init.zeros_(self.weights)
        elif init_method == 'correlates' : 
            self.weights = torch.nn.Parameter(torch.relu_(centers + torch.normal(0,0.2,centers.shape,device=device,dtype=torch.float64)))
        else : 
            self.weights = torch.nn.Parameter(torch.empty(size=(nsigs,1),device=device,dtype=torch.float64))
            torch.nn.init.uniform_(self.weights,a=0,b=1)
        
    def forward(self,J,return_weights=False) :
        corrected_weight=self.relu(self.weights) ;
        out=torch.matmul(J,corrected_weight)
        if not return_weights : 
            return out
        else : 
            return out,corrected_weight
        
class AIC_Loss(torch.nn.Module): 
    
    def __init__(self,nv,device=torch.device('cpu')) : 
        super(AIC_Loss,self).__init__()
        self.mseloss=torch.nn.MSELoss()
        self.device=device
        self.nv=torch.tensor(nv,dtype=torch.float64,device=self.device)
        
    def forward(self,outputs,targets,Jweights) : 
        """
        https://en.wikipedia.org/wiki/Regularized_least_squares#Lasso_regression
        """
        
        n=outputs.shape[0]
        sse=self.mseloss(outputs.ravel(),targets)*n
        llterm=-1*n*torch.log(2*np.pi*self.nv)-sse/self.nv
        kay=(Jweights != 0).sum()
        
        return 2*kay-2*llterm
    
gen_optimizer=lambda model: torch.optim.AdamW(model.parameters(),lr=3e-1,weight_decay=1e-5)

# parameters of the optimization chain
NCHAINS=120
COOLDOWN=100
NEPOCHS=301
SAMPLERATE=10
SAMPLINGTIMES=[ e for e in range(nepochs) if (e > cooldown) and (e % samplerate ==0) ]

# logging tensors
NSAMPLINGS=len(SAMPLINGTIMES)

def run_chain_chain(thisy) : 

    llog=torch.empty(size=(NCHAINS,NSAMPLINGS),requires_grad=False,device=DEVICE,dtype=torch.float64)
    wlog=torch.empty(size=(NCHAINS,bigX.shape[1]),requires_grad=False,device=DEVICE,dtype=torch.float64)
    ttbx=torch.tensor(bigX,dtype=torch.float64,device=DEVICE)
    colindices=torch.arange(0,wlog.shape[1],device=DEVICE).reshape(-1,1)


#RESUME
#~~~~~~~~Below is pasted from notebook~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~










with open(opj(outpath,'all_models_pickled.pickle'),'wb') as f : 
    for x in ['orig']+['{:0>4}'.format(x) for x in range(1,int(ns.n_repeats)+1) ] : 
        if x == 'orig' : 
            thisy=y
        else: 
            thisy=np.log(omics.sample(frac=0.95,replace=False).sum(axis=0)+1) #230920
            #thisy=np.log10(omics.sample(frac=0.95,replace=False).sum(axis=0)+1) #230910
            #thisy=np.log10(omics.sample(frac=1,replace=True).sum(axis=0)+1)

        lmsg(x,lastroundstart) ;
        lastroundstart=time.time()


        with warnings.catch_warnings() :
            warnings.simplefilter('ignore')
            os.environ['PYTHONWARNINGS']='ignore'
            #mod=LassoLarsIC(criterion='aic',positive=True,noise_variance=0.1) #early
            #mod=LassoLarsIC(criterion='aic',positive=True,noise_variance=0.1,fit_intercept=False) #230910
            #mod=LassoLarsIC(criterion='aic',positive=False,noise_variance=0.1,fit_intercept=False) #230920
            mod=LassoLarsIC(criterion='aic',positive=True,noise_variance=NV,fit_intercept=False) #230921
            mod.fit(bigX,thisy) ; 
            
        aic=mod.criterion_.min()
        nnz=( mod.coef_ != 0).sum()
        nnz_systems=len(np.intersect1d(nkeys,np.array(bigxcols)[ mod.coef_ != 0]))

        odd={ 'hierarchy' : hname ,
              'run_kind'  : x,
              'aic' : aic,
              'nnz' : nnz ,
              'nnz_systems' : nnz_systems,
              'n_iter' : mod.n_iter_ }
        outdata.append(odd)

        pickle.dump(mod,f)

odf=pd.DataFrame(outdata)
odf.to_csv(opj(outpath,'models_summary_data.csv'))
with open(opj(outpath,'bigX.pickle'),'wb') as f : 
    pickle.dump((bigxcols,omics.columns,bigX),f)
